"""
Extract 5 core financial tables from 10-K PDF using AWS Textract.

Tables extracted:
  1. Consolidated Statements of Operations (Income Statement)
  2. Consolidated Statements of Comprehensive Income
  3. Consolidated Balance Sheets
  4. Consolidated Statements of Shareholders' Equity
  5. Consolidated Statements of Cash Flows
"""

import time
import uuid
import re
import streamlit as st
import boto3


# ══════════════════════════════════════════════════════════════════════════════
#  5 TABLE CATEGORIES — keyword dictionaries for matching
# ══════════════════════════════════════════════════════════════════════════════

TABLE_CATEGORIES = {
    "income_statement": {
        "display_name": "Consolidated Statements of Operations (Income Statement)",
        "keywords": [
            ("statement of operations", 5.0),
            ("statements of operations", 5.0),
            ("income statement", 5.0),
            ("statements of income", 5.0),
            ("statement of income", 5.0),
            ("statement of earnings", 5.0),
            ("net sales", 2.0),
            ("total net sales", 2.0),
            ("cost of sales", 2.0),
            ("cost of goods sold", 2.0),
            ("gross margin", 2.0),
            ("gross profit", 2.0),
            ("operating income", 1.5),
            ("operating expenses", 1.5),
            ("earnings per share", 2.0),
            ("net income", 1.5),
            ("provision for income tax", 1.5),
        ],
    },
    "comprehensive_income": {
        "display_name": "Consolidated Statements of Comprehensive Income",
        "keywords": [
            ("comprehensive income", 5.0),
            ("statements of comprehensive", 5.0),
            ("statement of comprehensive", 5.0),
            ("other comprehensive income", 4.0),
            ("other comprehensive loss", 4.0),
            ("total comprehensive income", 3.0),
            ("unrealized gains", 2.0),
            ("unrealized losses", 2.0),
            ("foreign currency translation", 2.0),
            ("derivative instruments", 1.5),
            ("marketable debt securities", 1.5),
            ("reclassification", 1.0),
        ],
    },
    "balance_sheet": {
        "display_name": "Consolidated Balance Sheets",
        "keywords": [
            ("balance sheet", 5.0),
            ("balance sheets", 5.0),
            ("statement of financial position", 5.0),
            ("statements of financial position", 5.0),
            ("financial position", 4.0),
            ("total assets", 3.0),
            ("total liabilities", 3.0),
            ("current assets", 2.0),
            ("current liabilities", 2.0),
            ("stockholders equity", 2.5),
            ("shareholders equity", 2.5),
            ("accounts receivable", 1.5),
            ("accounts payable", 1.5),
            ("retained earnings", 1.5),
            ("total equity", 2.0),
            ("property plant and equipment", 1.5),
        ],
    },
    "shareholders_equity": {
        "display_name": "Consolidated Statements of Shareholders' Equity",
        "keywords": [
            ("shareholders equity", 5.0),
            ("stockholders equity", 5.0),
            ("statement of equity", 5.0),
            ("statements of equity", 5.0),
            ("changes in equity", 4.0),
            ("changes in stockholders", 4.0),
            ("changes in shareholders", 4.0),
            ("common stock", 2.0),
            ("additional paid-in capital", 2.0),
            ("accumulated other comprehensive", 2.5),
            ("treasury stock", 2.0),
            ("beginning balances", 2.0),
            ("ending balances", 2.0),
            ("share repurchase", 1.0),
            ("dividends declared", 1.0),
            ("common stock repurchased", 1.5),
        ],
    },
    "cash_flow": {
        "display_name": "Consolidated Statements of Cash Flows",
        "keywords": [
            ("statement of cash flows", 5.0),
            ("statements of cash flows", 5.0),
            ("cash flow statement", 5.0),
            ("cash flows", 4.0),
            ("operating activities", 3.0),
            ("investing activities", 3.0),
            ("financing activities", 3.0),
            ("cash and cash equivalents", 2.0),
            ("depreciation and amortization", 1.5),
            ("capital expenditures", 1.5),
            ("repurchases of common stock", 1.5),
            ("dividends and dividend equivalents", 1.0),
            ("net cash", 2.0),
            ("cash generated by", 1.5),
            ("cash used in", 1.5),
        ],
    },
}

MIN_SCORE = 4.0


# ══════════════════════════════════════════════════════════════════════════════
#  AWS CLIENTS
# ══════════════════════════════════════════════════════════════════════════════

def _get_textract():
    return boto3.client(
        "textract",
        aws_access_key_id=st.secrets["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key=st.secrets["AWS_SECRET_ACCESS_KEY"],
        region_name=st.secrets["AWS_REGION"],
    )

def _get_s3():
    return boto3.client(
        "s3",
        aws_access_key_id=st.secrets["AWS_ACCESS_KEY_ID"],
        aws_secret_access_key=st.secrets["AWS_SECRET_ACCESS_KEY"],
        region_name=st.secrets["AWS_REGION"],
    )


# ══════════════════════════════════════════════════════════════════════════════
#  MAIN ENTRY POINT
# ══════════════════════════════════════════════════════════════════════════════

def extract_tables_from_pdf(pdf_bytes: bytes) -> dict:
    """
    Extract 5 core financial tables from PDF.

    Returns:
    {
      "income_statement": { "found": bool, "display_name": str, "page": int, "unit": str, "headers": [...], "rows": [...] },
      "comprehensive_income": { ... },
      "balance_sheet": { ... },
      "shareholders_equity": { ... },
      "cash_flow": { ... },
    }
    """
    bucket = st.secrets["S3_BUCKET"]
    temp_key = f"_textract_temp/{uuid.uuid4().hex}.pdf"
    s3 = _get_s3()
    textract = _get_textract()

    try:
        s3.put_object(Bucket=bucket, Key=temp_key, Body=pdf_bytes)

        response = textract.start_document_analysis(
            DocumentLocation={"S3Object": {"Bucket": bucket, "Name": temp_key}},
            FeatureTypes=["TABLES"],
        )
        job_id = response["JobId"]

        # Poll
        waited = 0
        while waited < 180:
            result = textract.get_document_analysis(JobId=job_id)
            if result["JobStatus"] in ("SUCCEEDED", "FAILED"):
                break
            time.sleep(4)
            waited += 4

        if result["JobStatus"] != "SUCCEEDED":
            return _empty_result()

        # Collect all blocks
        all_blocks = result.get("Blocks", [])
        nt = result.get("NextToken")
        while nt:
            result = textract.get_document_analysis(JobId=job_id, NextToken=nt)
            all_blocks.extend(result.get("Blocks", []))
            nt = result.get("NextToken")

        raw_tables = _parse_all_tables(all_blocks)
        if not raw_tables:
            return _empty_result()

        return _classify_and_format(raw_tables, all_blocks)

    finally:
        try:
            s3.delete_object(Bucket=bucket, Key=temp_key)
        except Exception:
            pass


def _empty_result():
    return {
        cat: {"found": False, "display_name": info["display_name"], "page": None, "unit": "", "headers": [], "rows": []}
        for cat, info in TABLE_CATEGORIES.items()
    }


# ══════════════════════════════════════════════════════════════════════════════
#  PHASE 1: PARSE RAW TABLES FROM TEXTRACT BLOCKS
# ══════════════════════════════════════════════════════════════════════════════

def _parse_all_tables(blocks):
    block_map = {b["Id"]: b for b in blocks}
    table_blocks = [b for b in blocks if b["BlockType"] == "TABLE"]

    tables = []
    for ti, tb in enumerate(table_blocks):
        page = tb.get("Page", 1)
        cell_ids = []
        for rel in tb.get("Relationships", []):
            if rel["Type"] == "CHILD":
                cell_ids.extend(rel["Ids"])

        cell_map = {}
        cell_tops = {}  # (row, col) -> top Y coordinate
        max_row, max_col = 0, 0
        for cid in cell_ids:
            cell = block_map.get(cid)
            if not cell or cell["BlockType"] != "CELL":
                continue
            ri, ci = cell.get("RowIndex", 1), cell.get("ColumnIndex", 1)
            max_row, max_col = max(max_row, ri), max(max_col, ci)
            cell_map[(ri, ci)] = _get_cell_text(cell, block_map)
            # Store vertical position for row-label matching
            geo = cell.get("Geometry", {}).get("BoundingBox", {})
            cell_tops[(ri, ci)] = geo.get("Top", 0)

        if max_row < 2 or max_col < 2:
            continue

        rows = []
        row_tops = {}  # row_index -> top Y
        for r in range(1, max_row + 1):
            rows.append([cell_map.get((r, c), "") for c in range(1, max_col + 1)])
            # Use first cell's top as row's Y position
            for c in range(1, max_col + 1):
                if (r, c) in cell_tops:
                    row_tops[r] = cell_tops[(r, c)]
                    break

        all_text = " ".join(v for v in cell_map.values()).lower()

        # Get table bounding box
        tbl_geo = tb.get("Geometry", {}).get("BoundingBox", {})
        tbl_left = tbl_geo.get("Left", 0)

        tables.append({
            "table_index": ti,
            "page": page,
            "max_row": max_row,
            "max_col": max_col,
            "rows": rows,
            "row_tops": row_tops,
            "tbl_left": tbl_left,
            "all_text": all_text,
        })

    return tables


def _get_cell_text(cell, block_map):
    words = []
    for rel in cell.get("Relationships", []):
        if rel["Type"] == "CHILD":
            for wid in rel["Ids"]:
                w = block_map.get(wid)
                if w and w["BlockType"] == "WORD":
                    words.append(w.get("Text", ""))
    return " ".join(words).strip()


# ══════════════════════════════════════════════════════════════════════════════
#  PHASE 2: CLASSIFY, FILTER, FORMAT
# ══════════════════════════════════════════════════════════════════════════════

def _get_nearby_lines(table, all_blocks, before=10):
    """Get LINE text near the table on the same page (for title/unit detection)."""
    page = table["page"]
    lines = [
        b.get("Text", "")
        for b in all_blocks
        if b["BlockType"] == "LINE" and b.get("Page", 1) == page
    ]
    return " ".join(lines).lower()


def _detect_unit(nearby_text: str) -> str:
    """Detect the unit from nearby text."""
    t = nearby_text.lower()
    if "in millions" in t:
        return "In millions"
    if "in thousands" in t:
        return "In thousands"
    if "in billions" in t:
        return "In billions"
    return ""


def _score_table(table, cat_key, nearby_text):
    keywords = TABLE_CATEGORIES[cat_key]["keywords"]
    combined = table["all_text"] + " " + nearby_text
    score = sum(w for kw, w in keywords if kw in combined)
    # Bonus for larger tables (real financial statements are big)
    if table["max_row"] >= 10:
        score += 1.0
    if table["max_row"] >= 20:
        score += 1.0
    return score


def _classify_and_format(raw_tables, all_blocks):
    # Pre-compute nearby text and page lines
    for t in raw_tables:
        t["nearby_text"] = _get_nearby_lines(t, all_blocks)

    # Build page LINE index with positions for row-label recovery
    page_lines_with_pos = {}
    for b in all_blocks:
        if b["BlockType"] == "LINE":
            pg = b.get("Page", 1)
            geo = b.get("Geometry", {}).get("BoundingBox", {})
            left = geo.get("Left", 0)
            top = geo.get("Top", 0)
            text = b.get("Text", "").strip()
            if text:
                if pg not in page_lines_with_pos:
                    page_lines_with_pos[pg] = []
                page_lines_with_pos[pg].append({"text": text, "left": left, "top": top})

    result = _empty_result()
    assigned = set()

    priority = ["income_statement", "comprehensive_income", "balance_sheet", "shareholders_equity", "cash_flow"]

    for cat in priority:
        best_score = 0
        best_table = None

        for t in raw_tables:
            if t["table_index"] in assigned:
                continue
            sc = _score_table(t, cat, t["nearby_text"])
            if sc > best_score and sc >= MIN_SCORE:
                best_score = sc
                best_table = t

        if best_table:
            assigned.add(best_table["table_index"])
            unit = _detect_unit(best_table["nearby_text"])
            rows = best_table["rows"]

            # ── Row-label recovery ────────────────────────────────────
            # Check if first column is mostly empty or only has numbers (missing labels)
            first_col = [r[0].strip() if r else "" for r in rows]
            non_empty_first = [v for v in first_col if v]

            # Count how many first-col values are purely numeric (dollar amounts)
            def is_numeric_cell(v):
                cleaned = v.replace("$", "").replace(",", "").replace("(", "").replace(")", "").replace(".", "").replace("-", "").replace(" ", "")
                return cleaned.isdigit() or cleaned == ""

            numeric_count = sum(1 for v in non_empty_first if is_numeric_cell(v))

            # If >60% of non-empty first-col values are numeric, labels are missing
            has_labels = True
            if non_empty_first:
                if numeric_count / len(non_empty_first) > 0.6:
                    has_labels = False
            else:
                has_labels = False  # first column entirely empty

            if not has_labels:
                rows = _recover_row_labels(
                    rows, best_table, page_lines_with_pos.get(best_table["page"], [])
                )

            # Separate headers from data
            headers = rows[0] if rows else []
            data_rows = rows[1:] if len(rows) > 1 else []

            result[cat] = {
                "found": True,
                "display_name": TABLE_CATEGORIES[cat]["display_name"],
                "page": best_table["page"],
                "unit": unit,
                "headers": headers,
                "rows": data_rows,
            }

    return result


def _recover_row_labels(rows, table, page_lines):
    """
    Recover missing row labels by matching LINE blocks to the left of the table
    with each table row based on vertical (Y) position.
    """
    if not page_lines or not rows:
        return rows

    row_tops = table.get("row_tops", {})
    tbl_left = table.get("tbl_left", 0.5)

    # Use lines that are to the LEFT of the table (with some margin)
    # Also include lines that slightly overlap the table left edge
    left_lines = [ln for ln in page_lines if ln["left"] < tbl_left + 0.01]

    if not left_lines:
        return rows

    # Sort left lines by vertical position
    left_lines.sort(key=lambda x: x["top"])

    # For each row, find the closest left-side LINE by vertical position
    new_rows = []
    used_lines = set()

    for ri, row in enumerate(rows):
        actual_ri = ri + 1  # 1-based index
        row_top = row_tops.get(actual_ri)

        # Estimate position if not available
        if row_top is None and row_tops:
            sorted_tops = sorted(row_tops.items())
            if len(sorted_tops) >= 2:
                min_ri, min_top = sorted_tops[0]
                max_ri, max_top = sorted_tops[-1]
                if max_ri > min_ri:
                    step = (max_top - min_top) / (max_ri - min_ri)
                    row_top = min_top + (actual_ri - min_ri) * step

        label = ""
        if row_top is not None:
            # Find the closest LINE within vertical tolerance
            tolerance = 0.025  # ~2.5% of page height
            best_dist = tolerance
            best_idx = -1
            for idx, ln in enumerate(left_lines):
                if idx in used_lines:
                    continue
                dist = abs(ln["top"] - row_top)
                if dist < best_dist:
                    best_dist = dist
                    best_idx = idx
                    label = ln["text"]

            if best_idx >= 0:
                used_lines.add(best_idx)

        # Prepend label as first column
        new_rows.append([label] + row)

    return new_rows
